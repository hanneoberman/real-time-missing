@article{2021a,
  ids = {2021b},
  title = {Pattern-{{Mixture Models}} for {{Multivariate Incomplete Data}}},
  year = {2021},
  pages = {11},
  langid = {english}
}

@article{austin2014,
  title = {Graphical Assessment of Internal and External Calibration of Logistic Regression Models by Using Loess Smoothers},
  author = {Austin, Peter C. and Steyerberg, Ewout W.},
  year = {2014},
  journal = {Statistics in Medicine},
  volume = {33},
  number = {3},
  pages = {517--535},
  issn = {1097-0258},
  doi = {10.1002/sim.5941},
  urldate = {2021-07-19},
  abstract = {Predicting the probability of the occurrence of a binary outcome or condition is important in biomedical research. While assessing discrimination is an essential issue in developing and validating binary prediction models, less attention has been paid to methods for assessing model calibration. Calibration refers to the degree of agreement between observed and predicted probabilities and is often assessed by testing for lack-of-fit. The objective of our study was to examine the ability of graphical methods to assess the calibration of logistic regression models. We examined lack of internal calibration, which was related to misspecification of the logistic regression model, and external calibration, which was related to an overfit model or to shrinkage of the linear predictor. We conducted an extensive set of Monte Carlo simulations with a locally weighted least squares regression smoother (i.e., the loess algorithm) to examine the ability of graphical methods to assess model calibration. We found that loess-based methods were able to provide evidence of moderate departures from linearity and indicate omission of a moderately strong interaction. Misspecification of the link function was harder to detect. Visual patterns were clearer with higher sample sizes, higher incidence of the outcome, or higher discrimination. Loess-based methods were also able to identify the lack of calibration in external validation samples when an overfit regression model had been used. In conclusion, loess-based smoothing methods are adequate tools to graphically assess calibration and merit wider application. \textcopyright{} 2013 The Authors. Statistics in Medicine published by John Wiley \& Sons, Ltd},
  langid = {english},
  keywords = {calibration,graphical methods,logistic regression,prediction,prediction models},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\AQYK8BVY\\Austin and Steyerberg - 2014 - Graphical assessment of internal and external cali.pdf;C\:\\Users\\4216318\\Zotero\\storage\\FCJ3QFVG\\sim.html}
}

@article{austin2019,
  title = {The {{Integrated Calibration Index}} ({{ICI}}) and Related Metrics for Quantifying the Calibration of Logistic Regression Models},
  author = {Austin, Peter C. and Steyerberg, Ewout W.},
  year = {2019},
  journal = {Statistics in Medicine},
  volume = {38},
  number = {21},
  pages = {4051--4065},
  issn = {1097-0258},
  doi = {10.1002/sim.8281},
  urldate = {2021-07-19},
  abstract = {Assessing the calibration of methods for estimating the probability of the occurrence of a binary outcome is an important aspect of validating the performance of risk-prediction algorithms. Calibration commonly refers to the agreement between predicted and observed probabilities of the outcome. Graphical methods are an attractive approach to assess calibration, in which observed and predicted probabilities are compared using loess-based smoothing functions. We describe the Integrated Calibration Index (ICI) that is motivated by Harrell's Emax index, which is the maximum absolute difference between a smooth calibration curve and the diagonal line of perfect calibration. The ICI can be interpreted as weighted difference between observed and predicted probabilities, in which observations are weighted by the empirical density function of the predicted probabilities. As such, the ICI is a measure of calibration that explicitly incorporates the distribution of predicted probabilities. We also discuss two related measures of calibration, E50 and E90, which represent the median and 90th percentile of the absolute difference between observed and predicted probabilities. We illustrate the utility of the ICI, E50, and E90 by using them to compare the calibration of logistic regression with that of random forests and boosted regression trees for predicting mortality in patients hospitalized with a heart attack. The use of these numeric metrics permitted for a greater differentiation in calibration than was permissible by visual inspection of graphical calibration curves.},
  langid = {english},
  keywords = {calibration,logistic regression,model validation},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\9IGA7EKH\\Austin and Steyerberg - 2019 - The Integrated Calibration Index (ICI) and related.pdf;C\:\\Users\\4216318\\Zotero\\storage\\FIWVBGC2\\sim.html}
}

@article{bezemer2019,
  ids = {bezemer2019a},
  title = {A {{Human}}(e) {{Factor}} in {{Clinical Decision Support Systems}}},
  author = {Bezemer, Tim and {de Groot}, Mark CH and Blasse, Enja and {ten Berg}, Maarten J and Kappen, Teus H and Bredenoord, Annelien L and {van Solinge}, Wouter W and Hoefer, Imo E and Haitjema, Saskia},
  year = {2019},
  month = mar,
  journal = {Journal of Medical Internet Research},
  volume = {21},
  number = {3},
  pages = {e11732},
  issn = {1438-8871},
  doi = {10.2196/11732},
  abstract = {The overwhelming amount, production speed, multidimensionality, and potential value of data currently available\textemdash often simplified and referred to as big data \textemdash exceed the limits of understanding of the human brain. At the same time, developments in data analytics and computational power provide the opportunity to obtain new insights and transfer data-provided added value to clinical practice in real time. What is the role of the health care professional in collaboration with the data scientist in the changing landscape of modern care? We discuss how health care professionals should provide expert knowledge in each of the stages of clinical decision support design: data level, algorithm level, and decision support level. Including various ethical considerations, we advocate for health care professionals to responsibly initiate and guide interprofessional teams, including patients, and embrace novel analytic technologies to translate big data into patient benefit driven by human(e) values.},
  langid = {english}
}

@book{breiman1984,
  ids = {breiman1984a},
  title = {Classification and {{Regression Trees}}},
  author = {Breiman, L. and Friedman, J. and Stone, C.J. and Olshen, R.A.},
  year = {1984},
  isbn = {978-0-412-04841-8}
}

@book{buur18,
  ids = {vanbuuren2018,vanbuuren2018a},
  title = {Flexible Imputation of Missing Data},
  author = {Van Buuren, Stef},
  year = {2018},
  publisher = {{Chapman and Hall/CRC}}
}

@article{cevallosvaldiviezo2015b,
  ids = {cevallosvaldiviezo2015,cevallosvaldiviezo2015a},
  title = {Tree-Based Prediction on Incomplete Data Using Imputation or Surrogate Decisions},
  author = {Cevallos Valdiviezo, H. and Van Aelst, S.},
  year = {2015},
  month = aug,
  journal = {Information Sciences},
  volume = {311},
  pages = {163--181},
  issn = {00200255},
  doi = {10.1016/j.ins.2015.03.018},
  urldate = {2021-06-16},
  abstract = {The goal is to investigate the prediction performance of tree-based techniques when the available training data contains features with missing values. Also the future test cases may contain missing values and thus the methods should be able to generate predictions for such test cases. The missing values are handled either by using surrogate decisions within the trees or by the combination of an imputation method with a tree-based method. Missing values generated according to missing completely at random (MCAR), missing at random (MAR) and missing not at random (MNAR) mechanisms are considered with various fractions of missing data. Imputation models are built in the learning phase and do not make use of the response variable, so that the resulting procedures allow to predict individual incomplete test cases. In the empirical comparison, both classification and regression problems are considered using a simulated and real-life datasets. The performance is evaluated by misclassification rate of predictions and mean squared prediction error, respectively. Overall, our results show that for smaller fractions of missing data an ensemble method combined with surrogates or single imputation suffices. For moderate to large fractions of missing values ensemble methods based on conditional inference trees combined with multiple imputation show the best performance, while conditional bagging using surrogates is a good alternative for high-dimensional prediction problems. Theoretical results confirm the potential better prediction performance of multiple imputation ensembles.},
  langid = {english},
  file = {C:\Users\4216318\Zotero\storage\3TZHAU4I\Cevallos Valdiviezo and Van Aelst - 2015 - Tree-based prediction on incomplete data using imp.pdf}
}

@article{collins2014,
  title = {External Validation of Multivariable Prediction Models: A Systematic Review of Methodological Conduct and Reporting},
  shorttitle = {External Validation of Multivariable Prediction Models},
  author = {Collins, Gary S. and {de Groot}, Joris A. and Dutton, Susan and Omar, Omar and Shanyinde, Milensu and Tajar, Abdelouahid and Voysey, Merryn and Wharton, Rose and Yu, Ly-Mee and Moons, Karel G. and Altman, Douglas G.},
  year = {2014},
  month = mar,
  journal = {BMC Medical Research Methodology},
  volume = {14},
  number = {1},
  pages = {40},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-14-40},
  urldate = {2021-07-19},
  abstract = {Before considering whether to use a multivariable (diagnostic or prognostic) prediction model, it is essential that its performance be evaluated in data that were not used to develop the model (referred to as external validation). We critically appraised the methodological conduct and reporting of external validation studies of multivariable prediction models.},
  keywords = {Clinical Prediction Model,Decision Curve Analysis,External Validation,External Validation Study,Integer Score},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\SLJRSGAV\\Collins et al. - 2014 - External validation of multivariable prediction mo.pdf;C\:\\Users\\4216318\\Zotero\\storage\\Q2F4Z2B5\\1471-2288-14-40.html}
}

@article{dagostino2008,
  ids = {dagostino2008a},
  title = {General {{Cardiovascular Risk Profile}} for {{Use}} in {{Primary Care}}: {{The Framingham Heart Study}}},
  author = {D'Agostino, Ralph B. and Vasan, Ramachandran S. and Pencina, Michael J. and Wolf, Philip A. and Cobain, Mark and Massaro, Joseph M. and Kannel, William B.},
  year = {2008},
  month = feb,
  journal = {Circulation},
  volume = {117},
  number = {6},
  pages = {743--753},
  issn = {0009-7322, 1524-4539},
  doi = {10.1161/CIRCULATIONAHA.107.699579},
  abstract = {Background\textemdash Separate multivariable risk algorithms are commonly used to assess risk of specific atherosclerotic cardiovascular disease (CVD) events, ie, coronary heart disease, cerebrovascular disease, peripheral vascular disease, and heart failure. The present report presents a single multivariable risk function that predicts risk of developing all CVD and of its constituents. Methods and Results\textemdash We used Cox proportional-hazards regression to evaluate the risk of developing a first CVD event in 8491 Framingham study participants (mean age, 49 years; 4522 women) who attended a routine examination between 30 and 74 years of age and were free of CVD. Sex-specific multivariable risk functions (``general CVD'' algorithms) were derived that incorporated age, total and high-density lipoprotein cholesterol, systolic blood pressure, treatment for hypertension, smoking, and diabetes status. We assessed the performance of the general CVD algorithms for predicting individual CVD events (coronary heart disease, stroke, peripheral artery disease, or heart failure). Over 12 years of follow-up, 1174 participants (456 women) developed a first CVD event. All traditional risk factors evaluated predicted CVD risk (multivariable-adjusted PϽ0.0001). The general CVD algorithm demonstrated good discrimination (C statistic, 0.763 [men] and 0.793 [women]) and calibration. Simple adjustments to the general CVD risk algorithms allowed estimation of the risks of each CVD component. Two simple risk scores are presented, 1 based on all traditional risk factors and the other based on non\textendash laboratory-based predictors. Conclusions\textemdash A sex-specific multivariable risk factor algorithm can be conveniently used to assess general CVD risk and risk of individual CVD events (coronary, cerebrovascular, and peripheral arterial disease and heart failure). The estimated absolute CVD event rates can be used to quantify risk and to guide preventive care. (Circulation. 2008;117: 743-753.)},
  langid = {english}
}

@article{donders2006b,
  ids = {donders2006,donders2006a},
  title = {Review: {{A}} Gentle Introduction to Imputation of Missing Values},
  shorttitle = {Review},
  author = {Donders, A. Rogier T. and {van der Heijden}, Geert J. M. G. and Stijnen, Theo and Moons, Karel G. M.},
  year = {2006},
  month = oct,
  journal = {Journal of Clinical Epidemiology},
  volume = {59},
  number = {10},
  pages = {1087--1091},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2006.01.014},
  urldate = {2021-07-06},
  abstract = {In most situations, simple techniques for handling missing data (such as complete case analysis, overall mean imputation, and the missing-indicator method) produce biased results, whereas imputation techniques yield valid results without complicating the analysis once the imputations are carried out. Imputation techniques are based on the idea that any subject in a study sample can be replaced by a new randomly chosen subject from the same source population. Imputation of missing data on a variable is replacing that missing by a value that is drawn from an estimate of the distribution of this variable. In single imputation, only one estimate is used. In multiple imputation, various estimates are used, reflecting the uncertainty in the estimation of this distribution. Under the general conditions of so-called missing at random and missing completely at random, both single and multiple imputations result in unbiased estimates of study associations. But single imputation results in too small estimated standard errors, whereas multiple imputation results in correctly estimated standard errors and confidence intervals. In this article we explain why all this is the case, and use a simple simulation study to demonstrate our explanations. We also explain and illustrate why two frequently used methods to handle missing data, i.e., overall mean imputation and the missing-indicator method, almost always result in biased estimates.},
  langid = {english},
  keywords = {Bias,Indicator method,Missing data,Multiple imputation,Precision,Single imputation},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\K8QPTTEY\\Donders et al. - 2006 - Review A gentle introduction to imputation of mis.pdf;C\:\\Users\\4216318\\Zotero\\storage\\S8TJRWX9\\S0895435606001971.html}
}

@article{dorado-diaz2019,
  ids = {dorado-diaz2019a},
  title = {Applications of {{Artificial Intelligence}} in {{Cardiology}}. {{The Future}} Is {{Already Here}}},
  author = {{Dorado-D{\'i}az}, P. Ignacio and {Sampedro-G{\'o}mez}, Jes{\'u}s and {Vicente-Palacios}, V{\'i}ctor and S{\'a}nchez, Pedro L.},
  year = {2019},
  month = dec,
  journal = {Revista Espa\~nola de Cardiolog\'ia (English Edition)},
  volume = {72},
  number = {12},
  pages = {1065--1075},
  issn = {18855857},
  doi = {10.1016/j.rec.2019.05.014},
  langid = {english}
}

@article{dorresteijn2013,
  ids = {dorresteijn2013a},
  title = {Development and Validation of a Prediction Rule for Recurrent Vascular Events Based on a Cohort Study of Patients with Arterial Disease: The {{SMART}} Risk Score},
  author = {Dorresteijn, Johannes A N and Visseren, Frank L J and Wassink, Annemarie M J and Gondrie, Martijn J A and Steyerberg, Ewout W and Ridker, Paul M and Cook, Nancy R and {van der Graaf}, Yolanda and {on behalf of the SMART Study Group}},
  year = {2013},
  month = jun,
  journal = {Heart},
  volume = {99},
  number = {12},
  pages = {866--872},
  issn = {1355-6037, 1468-201X},
  doi = {10.1136/heartjnl-2013-303640},
  abstract = {Objectives To enable risk stratification of patients with various types of arterial disease by the development and validation of models for prediction of recurrent vascular event risk based on vascular risk factors, imaging or both. Design Prospective cohort study. Setting University Medical Centre. Patients 5788 patients referred with various clinical manifestations of arterial disease between January 1996 and February 2010. Main outcome measures 788 recurrent vascular events (ie, myocardial infarction, stroke or vascular death) that were observed during 4.7 (IQR 2.3 to 7.7) years' follow-up. Results Three Cox proportional hazards models for prediction of 10-year recurrent vascular event risk were developed based on age and sex in addition to clinical parameters (model A), carotid ultrasound findings (model B) or both (model C). Clinical parameters were medical history, current smoking, systolic blood pressure and laboratory biomarkers. In a separate part of the dataset, the concordance statistic of model A was 0.68 (95\% CI 0.64 to 0.71), compared to 0.64 (0.61 to 0.68) for model B and 0.68 (0.65 to 0.72) for model C. Goodness-of-fit and calibration of model A were adequate, also in separate subgroups of patients having coronary, cerebrovascular, peripheral artery or aneurysmal disease. Model A predicted {$<$}20\% risk in 59\% of patients, 20\textendash 30\% risk in 19\% and {$>$}30\% risk in 23\%. Conclusions Patients at high risk for recurrent vascular events can be identified based on readily available clinical characteristics.},
  langid = {english}
}

@incollection{feelders1999,
  ids = {feelders1999a},
  title = {Handling {{Missing Data}} in {{Trees}}: {{Surrogate Splits}} or {{Statistical Imputation}}?},
  booktitle = {Principles of {{Data Mining}} and {{Knowledge Discovery}}},
  author = {Feelders, Ad},
  editor = {{\.Z}ytkow, Jan M. and Rauch, Jan and {\.Z}ytkow, Jan M. and Rauch, Jan},
  year = {1999},
  volume = {1704},
  pages = {329--334},
  address = {{Berlin, Heidelberg}},
  urldate = {2019-10-02},
  abstract = {In many applications of data mining a - sometimes considerable - part of the data values is missing. This may occur because the data values were simply never entered into the operational systems from which the mining table was constructed, or because for example simple domain checks indicate that entered values are incorrect. Despite the frequent occurrence of missing data, most data mining algorithms handle missing data in a rather ad-hoc way, or simply ignore the problem.},
  isbn = {978-3-540-66490-1},
  langid = {english}
}

@inproceedings{feelders1999b,
  ids = {feeldersHandlingMissingData1999a},
  title = {Handling {{Missing Data}} in {{Trees}}: {{Surrogate Splits}} or {{Statistical Imputation}}?},
  shorttitle = {Handling {{Missing Data}} in {{Trees}}},
  booktitle = {Principles of {{Data Mining}} and {{Knowledge Discovery}}},
  author = {Feelders, Ad},
  editor = {{\.Z}ytkow, Jan M. and Rauch, Jan},
  year = {1999},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {329--334},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-48247-5_38},
  abstract = {In many applications of data mining a \textendash{} sometimes considerable \textendash{} part of the data values is missing. Despite the frequent occurrence of missing data, most data mining algorithms handle missing data in a rather ad-hoc way, or simply ignore the problem. We investigate simulation-based data augmentation to handle missing data, which is based on filling-in (imputing) one or more plausible values for the missing data. One advantage of this approach is that the imputation phase is separated from the analysis phase, allowing for different data mining algorithms to be applied to the completed data sets. We compare the use of imputation to surrogate splits, such as used in CART, to handle missing data in tree-based mining algorithms. Experiments show that imputation tends to outperform surrogate splits in terms of predictive accuracy of the resulting models. Averaging over M {$>$} 1 models resulting from M imputations yields even better results as it profits from variance reduction in much the same way as procedures such as bagging.},
  isbn = {978-3-540-48247-5},
  langid = {english},
  keywords = {Data Augmentation,Data Mining Algorithm,Imputation Model,Multiple Imputation,Variance Reduction},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\D8YSMKEA\\Feelders - 1999 - Handling Missing Data in Trees Surrogate Splits o.pdf;C\:\\Users\\4216318\\Zotero\\storage\\ZQIM8TQM\\Feelders - 1999 - Handling Missing Data in Trees Surrogate Splits o.pdf}
}

@article{fletchermercaldo2020,
  ids = {fletchermercaldo2020a},
  title = {Missing Data and Prediction: The Pattern Submodel},
  author = {Fletcher Mercaldo, Sarah and Blume, Jeffrey D},
  year = {2020},
  month = apr,
  journal = {Biostatistics},
  volume = {21},
  number = {2},
  pages = {236--252},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxy040},
  abstract = {Missing data are a common problem for both the construction and implementation of a prediction algorithm. Pattern submodels (PS)\textemdash a set of submodels for every missing data pattern that are fit using only data from that pattern\textemdash are a computationally efficient remedy for handling missing data at both stages. Here, we show that PS (i) retain their predictive accuracy even when the missing data mechanism is not missing at random (MAR) and (ii) yield an algorithm that is the most predictive among all standard missing data strategies. Specifically, we show that the expected loss of a forecasting algorithm is minimized when each pattern-specific loss is minimized. Simulations and a re-analysis of the SUPPORT study confirms that PS generally outperforms zero-imputation, mean-imputation, complete-case analysis, complete-case submodels, and even multiple imputation (MI). The degree of improvement is highly dependent on the missingness mechanism and the effect size of missing predictors. When the data are MAR, MI can yield comparable forecasting performance but generally requires a larger computational cost. We also show that predictions from the PS approach are equivalent to the limiting predictions for a MI procedure that is dependent on missingness indicators (the MIMI model). The focus of this article is on out-of-sample prediction; implications for model inference are only briefly explored.},
  langid = {english}
}

@incollection{glynn1986,
  ids = {glynn1986a},
  title = {Selection {{Modeling Versus Mixture Modeling}} with {{Nonignorable Nonresponse}}},
  booktitle = {Drawing {{Inferences}} from {{Self-Selected Samples}}},
  author = {Glynn, Robert J. and Laird, Nan M. and Rubin, Donald B.},
  editor = {Wainer, Howard and Wainer, Howard},
  year = {1986},
  pages = {115--142},
  address = {{New York, NY}},
  abstract = {It is sometimes suspected that nonresponse to a sample survey is related to the primary outcome variable. This is the case, for example, in studies of income or of alcohol consumption behaviors. If nonresponse to a survey is related to the level of the outcome variable, then the sample mean of this outcome variable based on the respondents will generally be a biased estimate of the population mean. If this outcome variable has a linear regression on certain predictor variables in the population, then ordinary least squares estimates of the regression coefficients based on the responding units will generally be biased unless nonresponse is a stochastic function of these predictor variables. The purpose of this paper is to discuss the performance of two alternative approaches, the selection model approach and the mixture model approach, for obtaining estimates of means and regression estimates when nonresponse depends on the outcome variable. Both approaches extend readily to the situation when values of the outcome variable are available for a subsample of the nonrespondents, called ``follow-ups.'' The availability of follow-ups are a feature of the example we use to illustrate comparisons.},
  isbn = {978-1-4612-4976-4}
}

@article{grant2018,
  title = {Statistical {{Primer}}: Developing and Validating a Risk Prediction Model},
  shorttitle = {Statistical {{Primer}}},
  author = {Grant, Stuart W. and Collins, Gary S. and Nashef, Samer A. M.},
  year = {2018},
  month = aug,
  journal = {European Journal of Cardio-Thoracic Surgery: Official Journal of the European Association for Cardio-Thoracic Surgery},
  volume = {54},
  number = {2},
  pages = {203--208},
  issn = {1873-734X},
  doi = {10.1093/ejcts/ezy180},
  abstract = {A risk prediction model is a mathematical equation that uses patient risk factor data to estimate the probability of a patient experiencing a healthcare outcome. Risk prediction models are widely studied in the cardiothoracic surgical literature with most developed using logistic regression. For a risk prediction model to be useful, it must have adequate discrimination, calibration, face validity and clinical usefulness. A basic understanding of the advantages and potential limitations of risk prediction models is vital before applying them in clinical practice. This article provides a brief overview for the clinician on the various issues to be considered when developing or validating a risk prediction model. An example of how to develop a simple model is also included.},
  langid = {english},
  pmid = {29741602},
  keywords = {Aged,Calibration,Female,Humans,Male,Middle Aged,{Models, Statistical},Risk Assessment,ROC Curve,Thoracic Surgical Procedures},
  file = {C:\Users\4216318\Zotero\storage\XQ245KGR\Grant et al. - 2018 - Statistical Primer developing and validating a ri.pdf}
}

@article{groenhof2019,
  ids = {groenhof2019a},
  title = {A Computerised Decision Support System for Cardiovascular Risk Management `Live' in the Electronic Health Record Environment: Development, Validation and Implementation\textemdash the {{Utrecht Cardiovascular Cohort Initiative}}},
  author = {Groenhof, T. K. J. and Bots, M. L. and Brandjes, M. and Jacobs, J. J. L. and Grobbee, D. E. and {van Solinge}, W. W. and Visseren, F. L. J. and Haitjema, S. and Asselbergs, F. W.},
  year = {2019},
  month = sep,
  journal = {Netherlands Heart Journal},
  volume = {27},
  number = {9},
  pages = {435--442},
  issn = {1568-5888, 1876-6250},
  doi = {10.1007/s12471-019-01308-w},
  abstract = {Purpose We set out to develop a real-time computerised decision support system (CDSS) embedded in the electronic health record (EHR) with information on risk factors, estimated risk, and guideline-based advice on treatment strategy in order to improve adherence to cardiovascular risk management (CVRM) guidelines with the ultimate aim of improving patient healthcare.},
  langid = {english}
}

@book{hapfelmeier2012,
  ids = {hapfelmeier2012a},
  title = {Analysis of {{Missing Data}} with {{Random Forests}}},
  author = {Hapfelmeier, Alexander},
  year = {2012},
  urldate = {2019-09-04}
}

@phdthesis{hapfelmeier2012b,
  type = {{Text.PhDThesis}},
  title = {{Analysis of missing data with random forests}},
  author = {Hapfelmeier, Alexander},
  year = {2012},
  month = oct,
  urldate = {2021-05-12},
  abstract = {Random Forests are widely used for data prediction and interpretation purposes.  They show many appealing characteristics, such as the ability to deal with high dimensional data, complex interactions and correlations. Furthermore, missing values can easily be processed by the built-in procedure of surrogate splits. However, there is only little knowledge about the properties of recursive partitioning in missing data situations.  Therefore, extensive simulation studies and empirical evaluations have been conducted to gain deeper insight. In addition, new methods have been developed to enhance methodology and solve current issues of data interpretation, prediction and variable selection. A variable's relevance in a Random Forest can be assessed by means of importance measures. Unfortunately, existing methods cannot be applied when the data contain miss- ing values.  Thus, one of the most appreciated properties of Random Forests \textendash{} its ability to handle missing values \textendash{} gets lost for the computation of such measures.   This work presents a new approach that is designed to deal with missing values in an intuitive and straightforward way, yet retains widely appreciated qualities of existing methods. Results indicate that it meets sensible requirements and shows good variable ranking properties. Random Forests provide variable selection that is usually based on importance mea- sures.  An extensive review of corresponding literature led to the development of a new approach that is based on a profound theoretical framework and meets important statis- tical properties.  A comparison to another eight popular methods showed that it controls the test-wise and family-wise error rate, provides a higher power to distinguish relevant from non-relevant variables and leads to models located among the best performing ones. Alternative ways to handle missing values are the application of imputation methods and complete case analysis.  Yet it is unknown to what extent these approaches are able to provide sensible variable rankings and meaningful variable selections.   Investigations showed that complete case analysis leads to inaccurate variable selection as it may in- appropriately penalize the importance of fully observed variables.  By contrast, the new importance measure decreases for variables with missing values and therefore causes se- lections that accurately reflect the information given in actual data situations.  Multiple imputation leads to an assessment of a variable's importance and to selection frequencies that would be expected for data that was completely observed.  In several performance evaluations the best prediction accuracy emerged from multiple imputation, closely fol- lowed by the application of surrogate splits.   Complete case analysis clearly performed worst.},
  langid = {ngerman},
  school = {Ludwig-Maximilians-Universit\"at M\"unchen},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\CRLH89SN\\Hapfelmeier - 2012 - Analysis of missing data with random forests.pdf;C\:\\Users\\4216318\\Zotero\\storage\\4YMQSXN8\\15058.html}
}

@article{hoogland2020,
  ids = {hoogland2020a},
  title = {Handling Missing Predictor Values When Validating and Applying a Prediction Model to New Patients},
  author = {Hoogland, Jeroen and Barreveld, Marit and Debray, Thomas P. A. and Reitsma, Johannes B. and Verstraelen, Tom E. and Dijkgraaf, Marcel G. W. and Zwinderman, Aeilko H.},
  year = {2020},
  month = jul,
  journal = {Statistics in Medicine},
  pages = {sim.8682},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.8682},
  abstract = {Missing data present challenges for development and real-world application of clinical prediction models. While these challenges have received considerable attention in the development setting, there is only sparse research on the handling of missing data in applied settings. The main unique feature of handling missing data in these settings is that missing data methods have to be performed for a single new individual, precluding direct application of mainstay methods used during model development. Correspondingly, we propose that it is desirable to perform model validation using missing data methods that transfer to practice in single new patients. This article compares existing and new methods to account for missing data for a new individual in the context of prediction. These methods are based on (i) submodels based on observed data only, (ii) marginalization over the missing variables, or (iii) imputation based on fully conditional specification (also known as chained equations). They were compared in an internal validation setting to highlight the use of missing data methods that transfer to practice while validating a model. As a reference, they were compared to the use of multiple imputation by chained equations in a set of test patients, because this has been used in validation studies in the past. The methods were evaluated in a simulation study where performance was measured by means of optimism corrected C-statistic and mean squared prediction error. Furthermore, they were applied in data from a large Dutch cohort of prophylactic implantable cardioverter defibrillator patients.},
  langid = {english}
}

@article{hothorn2006,
  ids = {zotero-6270},
  title = {Unbiased {{Recursive Partitioning}}: {{A Conditional Inference Framework}}},
  shorttitle = {Unbiased {{Recursive Partitioning}}},
  author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
  year = {2006},
  month = sep,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {15},
  number = {3},
  pages = {651--674},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1198/106186006X133933},
  urldate = {2021-06-22},
  abstract = {Recursive binary partitioning is a popular tool for regression analysis. Two fundamental problems of exhaustive search procedures usually applied to fit such models have been known for a long time: overfitting and a selection bias towards covariates with many possible splits or missing values. While pruning procedures are able to solve the overfitting problem, the variable selection bias still seriously affects the interpretability of tree-structured regression models. For some special cases unbiased procedures have been suggested, however lacking a common theoretical foundation. We propose a unified framework for recursive partitioning which embeds tree-structured regression models into a well defined theory of conditional inference procedures. Stopping criteria based on multiple test procedures are implemented and it is shown that the predictive performance of the resulting trees is as good as the performance of established exhaustive search procedures. It turns out that the partitions and therefore the models induced by both approaches are structurally different, confirming the need for an unbiased variable selection. Moreover, it is shown that the prediction accuracy of trees with early stopping is equivalent to the prediction accuracy of pruned trees with unbiased variable selection. The methodology presented here is applicable to all kinds of regression problems, including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary measurement scales of the covariates. Data from studies on glaucoma classification, node positive breast cancer survival and mammography experience are re-analyzed.},
  keywords = {Multiple testing,Multivariate regression trees,Ordinal regression trees,Permutation tests,Variable selection},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\H9E5N62I\\Hothorn et al. - 2006 - Unbiased Recursive Partitioning A Conditional Inf.pdf;C\:\\Users\\4216318\\Zotero\\storage\\35TP72SG\\106186006X133933.html}
}

@article{ishwaran2008,
  title = {Random Survival Forests},
  author = {Ishwaran, Hemant and Kogalur, Udaya B. and Blackstone, Eugene H. and Lauer, Michael S.},
  year = {2008},
  month = sep,
  journal = {The Annals of Applied Statistics},
  volume = {2},
  number = {3},
  pages = {841--860},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/08-AOAS169},
  urldate = {2021-05-12},
  abstract = {We introduce random survival forests, a random forests method for the analysis of right-censored survival data. New survival splitting rules for growing survival trees are introduced, as is a new missing data algorithm for imputing missing data. A conservation-of-events principle for survival forests is introduced and used to define ensemble mortality, a simple interpretable measure of mortality that can be used as a predicted outcome. Several illustrative examples are given, including a case study of the prognostic implications of body mass for individuals with coronary artery disease. Computations for all examples were implemented using the freely available R-software package, randomSurvivalForest.},
  keywords = {Conservation of events,cumulative hazard function,ensemble,out-of-bag,prediction error,survival tree},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\XHWLZUHP\\Ishwaran et al. - 2008 - Random survival forests.pdf;C\:\\Users\\4216318\\Zotero\\storage\\SY83HUIE\\08-AOAS169.html}
}

@article{janssen2009,
  ids = {janssen2009a},
  title = {Dealing with {{Missing Predictor Values When Applying Clinical Prediction Models}}},
  author = {Janssen, Kristel J M and Vergouwe, Yvonne and Donders, A Rogier T and Harrell, Frank E and Chen, Qingxia and Grobbee, Diederick E and Moons, Karel G M},
  year = {2009},
  month = may,
  journal = {Clinical Chemistry},
  volume = {55},
  number = {5},
  pages = {994--1001},
  issn = {0009-9147, 1530-8561},
  doi = {10.1373/clinchem.2008.115345},
  abstract = {BACKGROUND: Prediction models combine patient characteristics and test results to predict the presence of a disease or the occurrence of an event in the future. In the event that test results (predictor) are unavailable, a strategy is needed to help users applying a prediction model to deal with such missing values. We evaluated 6 strategies to deal with missing values. METHODS: We developed and validated (in 1295 and 532 primary care patients, respectively) a prediction model to predict the risk of deep venous thrombosis. In an application set (259 patients), we mimicked 3 situations in which (1) an important predictor (D-dimer test), (2) a weaker predictor (difference in calf circumference), and (3) both predictors simultaneously were missing. The 6 strategies to deal with missing values were (1) ignoring the predictor, (2) overall mean imputation, (3) subgroup mean imputation, (4) multiple imputation, (5) applying a submodel including only the observed predictors as derived from the development set, or (6) the ``one-step-sweep'' method. We compared the model's discriminative ability (expressed by the ROC area) with the true ROC area (no missing values) and the model's estimated calibration slope and intercept with the ideal values of 1 and 0, respectively. RESULTS: Ignoring the predictor led to the worst and multiple imputation to the best discrimination. Multiple imputation led to calibration intercepts closest to the true value. The effect of the strategies on the slope differed between the 3 scenarios. CONCLUSIONS: Multiple imputation is preferred if a predictor value is missing. \textcopyright{} 2009 American Association for Clinical Chemistry},
  langid = {english}
}

@article{kolassa2020,
  title = {Why the ``Best'' Point Forecast Depends on the Error or Accuracy Measure},
  author = {Kolassa, Stephan},
  year = {2020},
  month = jan,
  journal = {International Journal of Forecasting},
  series = {M4 {{Competition}}},
  volume = {36},
  number = {1},
  pages = {208--211},
  issn = {0169-2070},
  doi = {10.1016/j.ijforecast.2019.02.017},
  urldate = {2021-11-22},
  langid = {english}
}

@article{nijman2021,
  ids = {nijman2021c},
  title = {Real-Time Imputation of Missing Predictor Values in Clinical Practice},
  author = {Nijman, Steven W J and Hoogland, Jeroen and Groenhof, T Katrien J and Brandjes, Menno and Jacobs, John J L and Bots, Michiel L and Asselbergs, Folkert W and Moons, Karel G M and Debray, Thomas P A},
  year = {2021},
  month = may,
  journal = {European Heart Journal - Digital Health},
  volume = {2},
  number = {1},
  pages = {154--164},
  issn = {2634-3916},
  doi = {10.1093/ehjdh/ztaa016},
  abstract = {Abstract Aims Use of prediction models is widely recommended by clinical guidelines, but usually requires complete information on all predictors, which is not always available in daily practice. We aim to describe two methods for real-time handling of missing predictor values when using prediction models in practice. Methods and results We compare the widely used method of mean imputation (M-imp) to a method that personalizes the imputations by taking advantage of the observed patient characteristics. These characteristics may include both prediction model variables and other characteristics (auxiliary variables). The method was implemented using imputation from a joint multivariate normal model of the patient characteristics (joint modelling imputation; JMI). Data from two different cardiovascular cohorts with cardiovascular predictors and outcome were used to evaluate the real-time imputation methods. We quantified the prediction model's overall performance [mean squared error (MSE) of linear predictor], discrimination (c-index), calibration (intercept and slope), and net benefit (decision curve analysis). When compared with mean imputation, JMI substantially improved the MSE (0.10 vs. 0.13), c-index (0.70 vs. 0.68), and calibration (calibration-in-the-large: 0.04 vs. 0.06; calibration slope: 1.01 vs. 0.92), especially when incorporating auxiliary variables. When the imputation method was based on an external cohort, calibration deteriorated, but discrimination remained similar. Conclusions We recommend JMI with auxiliary variables for real-time imputation of missing values, and to update imputation models when implementing them in new settings or (sub)populations.},
  langid = {english}
}

@article{nijman2021a,
  ids = {nijman2021b,nijman2021d,nijman2021e},
  title = {Real-Time Imputation of Missing Predictor Values Improved the Application of Prediction Models in Daily Practice},
  author = {Nijman, Steven Willem Joost and Groenhof, T. Katrien J. and Hoogland, Jeroen and Bots, Michiel L. and Brandjes, Menno and Jacobs, John J.L. and Asselbergs, Folkert W. and Moons, Karel G.M. and Debray, Thomas P.A.},
  year = {2021},
  month = jun,
  journal = {Journal of Clinical Epidemiology},
  volume = {134},
  pages = {22--34},
  issn = {08954356},
  doi = {10.1016/j.jclinepi.2021.01.003},
  abstract = {Objectives: In clinical practice, many prediction models cannot be used when predictor values are missing. We, therefore, propose and evaluate methods for real-time imputation. Study Design and Setting: We describe (i) mean imputation (where missing values are replaced by the sample mean), (ii) joint modeling imputation (JMI, where we use a multivariate normal approximation to generate patient-specific imputations), and (iii) conditional modeling imputation (CMI, where a multivariable imputation model is derived for each predictor from a population). We compared these methods in a case study evaluating the root mean squared error (RMSE) and coverage of the 95\% confidence intervals (i.e., the proportion of confidence intervals that contain the true predictor value) of imputed predictor values. Results: eRMSE was lowest when adopting JMI or CMI, although imputation of individual predictors did not always lead to substantial improvements as compared to mean imputation. JMI and CMI appeared particularly useful when the values of multiple predictors of the model were missing. Coverage reached the nominal level (i.e., 95\%) for both CMI and JMI. Conclusion: Multiple imputations using either CMI or JMI is recommended when dealing with missing predictor values in real-time settings. \'O 2021 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons. org/licenses/by/4.0/).},
  langid = {english}
}

@article{perperoglou2019,
  title = {A Review of Spline Function Procedures in {{R}}},
  author = {Perperoglou, Aris and Sauerbrei, Willi and Abrahamowicz, Michal and Schmid, Matthias},
  year = {2019},
  month = mar,
  journal = {BMC Medical Research Methodology},
  volume = {19},
  number = {1},
  pages = {46},
  issn = {1471-2288},
  doi = {10.1186/s12874-019-0666-3},
  urldate = {2021-10-05},
  abstract = {With progress on both the theoretical and the computational fronts the use of spline modelling has become an established tool in statistical regression analysis. An important issue in spline modelling is the availability of user friendly, well documented software packages. Following the idea of the STRengthening Analytical Thinking for Observational Studies initiative to provide users with guidance documents on the application of statistical methods in observational research, the aim of this article is to provide an overview of the most widely used spline-based techniques and their implementation in R.},
  keywords = {Functional form of continuous covariates,Multivariable modelling},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\SRSW6S69\\Perperoglou et al. - 2019 - A review of spline function procedures in R.pdf;C\:\\Users\\4216318\\Zotero\\storage\\A37YPAKQ\\s12874-019-0666-3.html}
}

@article{piepoli2016,
  ids = {piepoli2016a},
  title = {2016 {{European Guidelines}} on Cardiovascular Disease Prevention in Clinical Practice: {{The Sixth Joint Task Force}} of the {{European Society}} of {{Cardiology}} and {{Other Societies}} on {{Cardiovascular Disease Prevention}} in {{Clinical Practice}} (Constituted by Representatives of 10 Societies and by Invited Experts){{Developed}} with the Special Contribution of the {{European Association}} for {{Cardiovascular Prevention}} \& {{Rehabilitation}} ({{EACPR}})},
  author = {Piepoli, Massimo F. and Hoes, Arno W. and Agewall, Stefan and Albus, Christian and Brotons, Carlos and Catapano, Alberico L. and Cooney, Marie-Therese and Corr{\`a}, Ugo and Cosyns, Bernard and Deaton, Christi and Graham, Ian and Hall, Michael Stephen and Hobbs, F. D. Richard and L{\o}chen, Maja-Lisa and L{\"o}llgen, Herbert and {Marques-Vidal}, Pedro and Perk, Joep and Prescott, Eva and Redon, Josep and Richter, Dimitrios J. and Sattar, Naveed and Smulders, Yvo and Tiberi, Monica and {van der Worp}, H. Bart and {van Dis}, Ineke and Verschuren, W. M. Monique},
  year = {2016},
  month = aug,
  journal = {European Heart Journal},
  volume = {37},
  number = {29},
  pages = {2315--2381},
  issn = {0195-668X, 1522-9645},
  doi = {10.1093/eurheartj/ehw106},
  langid = {english}
}

@article{probst2019,
  title = {Hyperparameters and Tuning Strategies for Random Forest},
  author = {Probst, Philipp and Wright, Marvin N. and Boulesteix, Anne-Laure},
  year = {2019},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {9},
  number = {3},
  pages = {e1301},
  issn = {1942-4795},
  doi = {10.1002/widm.1301},
  urldate = {2021-06-09},
  abstract = {The random forest (RF) algorithm has several hyperparameters that have to be set by the user, for example, the number of observations drawn randomly for each tree and whether they are drawn with or without replacement, the number of variables drawn randomly for each split, the splitting rule, the minimum number of samples that a node must contain, and the number of trees. In this paper, we first provide a literature review on the parameters' influence on the prediction performance and on variable importance measures. It is well known that in most cases RF works reasonably well with the default values of the hyperparameters specified in software packages. Nevertheless, tuning the hyperparameters can improve the performance of RF. In the second part of this paper, after a presenting brief overview of tuning strategies, we demonstrate the application of one of the most established tuning strategies, model-based optimization (MBO). To make it easier to use, we provide the tuneRanger R package that tunes RF with MBO automatically. In a benchmark study on several datasets, we compare the prediction performance and runtime of tuneRanger with other tuning implementations in R and RF with default hyperparameters. This article is categorized under: Algorithmic Development {$>$} Biological Data Mining Algorithmic Development {$>$} Statistics Algorithmic Development {$>$} Hierarchies and Trees Technologies {$>$} Machine Learning},
  copyright = {\textcopyright{} 2019 Wiley Periodicals, Inc.},
  langid = {english},
  keywords = {ensemble,literature review,out-of-bag,performance evaluation,ranger,sequential model-based optimization,tuning parameter},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\8MDMIZG5\\Probst et al. - 2019 - Hyperparameters and tuning strategies for random f.pdf;C\:\\Users\\4216318\\Zotero\\storage\\FYZSC43Z\\widm.html}
}

@article{rubi76,
  ids = {rubin1976,rubin1976a,rubin1976inference},
  title = {Inference and {{Missing Data}}},
  author = {Rubin, Donald B.},
  year = {1976},
  journal = {Biometrika},
  volume = {63},
  number = {3},
  pages = {581--592},
  publisher = {{Biometrika Trust}},
  doi = {10.2307/2335739},
  urldate = {2020-04-03},
  abstract = {When making sampling distribution inferences about the parameter of the data, \texttheta, it is appropriate to ignore the process that causes missing data if the missing data are `missing at random' and the observed data are `observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about \texttheta, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is `distinct' from \texttheta. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
  date-added = {2016-01-31 19:05:50 +0000},
  date-modified = {2016-01-31 19:05:50 +0000},
  file = {C:\Users\4216318\Zotero\storage\XIZEI53J\Rubin - 1976 - Inference and Missing Data.pdf}
}

@article{schouten2018b,
  ids = {ampute,schouten2018,schouten2018a},
  title = {Generating Missing Values for Simulation Purposes: A Multivariate Amputation Procedure},
  shorttitle = {Generating Missing Values for Simulation Purposes},
  author = {Schouten, Rianne Margaretha and Lugtig, Peter and Vink, Gerko},
  year = {2018},
  month = oct,
  journal = {Journal of Statistical Computation and Simulation},
  volume = {88},
  number = {15},
  pages = {2909--2930},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10.1080/00949655.2018.1491577},
  urldate = {2021-11-02},
  abstract = {Missing data form a ubiquitous problem in scientific research, especially since most statistical analyses require complete data. To evaluate the performance of methods dealing with missing data, researchers perform simulation studies. An important aspect of these studies is the generation of missing values in a simulated, complete data set: the amputation procedure. We investigated the methodological validity and statistical nature of both the current amputation practice and a newly developed and implemented multivariate amputation procedure. We found that the current way of practice may not be appropriate for the generation of intuitive and reliable missing data problems. The multivariate amputation procedure, on the other hand, generates reliable amputations and allows for a proper regulation of missing data problems. The procedure has additional features to generate any missing data scenario precisely as intended. Hence, the multivariate amputation procedure is an efficient method to accurately evaluate missing data methodology.},
  keywords = {evaluation,Missing data,multiple imputation,multivariate amputation},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\JAWBPURX\\Schouten et al. - 2018 - Generating missing values for simulation purposes.pdf;C\:\\Users\\4216318\\Zotero\\storage\\HDPFSVPZ\\00949655.2018.html}
}

@article{stevens2020,
  ids = {stevensValidationClinicalPrediction2020a},
  title = {Validation of Clinical Prediction Models: What Does the ``Calibration Slope'' Really Measure?},
  shorttitle = {Validation of Clinical Prediction Models},
  author = {Stevens, Richard J. and Poppe, Katrina K.},
  year = {2020},
  month = feb,
  journal = {Journal of Clinical Epidemiology},
  volume = {118},
  pages = {93--99},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2019.09.016},
  urldate = {2021-07-19},
  abstract = {Background and Objectives Definitions of calibration, an aspect of model validation, have evolved over time. We examine use and interpretation of the statistic currently referred to as the calibration slope. Methods The history of the term ``calibration slope'', and usage in papers published in 2016 and 2017, were reviewed. The behaviour of the slope in illustrative hypothetical examples and in two examples in the clinical literature was demonstrated. Results The paper in which the statistic was proposed described it as a measure of ``spread'' and did not use the term ``calibration''. In illustrative examples, slope of 1 can be associated with good or bad calibration, and this holds true across different definitions of calibration. In data extracted from a previous study, the slope was correlated with discrimination, not overall calibration. Many authors of recent papers interpret the slope as a measure of calibration; a minority interpret it as a measure of discrimination or do not explicitly categorise it as either. Seventeen of thirty-three papers used the slope as the sole measure of calibration. Conclusion Misunderstanding about this statistic has led to many papers in which it is the sole measure of calibration, which should be discouraged.},
  langid = {english},
  keywords = {Calibration,Clinical prediction rule,Discrimination,Slope,Spread,Validation},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\63ENLXWN\\Stevens and Poppe - 2020 - Validation of clinical prediction models what doe.pdf;C\:\\Users\\4216318\\Zotero\\storage\\ASWJXGBV\\Stevens and Poppe - 2020 - Validation of clinical prediction models what doe.pdf}
}

@article{steyerberg2014,
  ids = {steyerberg2014a},
  title = {Towards Better Clinical Prediction Models: Seven Steps for Development and an {{ABCD}} for Validation},
  author = {Steyerberg, E. W. and Vergouwe, Y.},
  year = {2014},
  month = aug,
  journal = {European Heart Journal},
  volume = {35},
  number = {29},
  pages = {1925--1931},
  issn = {0195-668X, 1522-9645},
  doi = {10.1093/eurheartj/ehu207},
  langid = {english}
}

@book{steyerberg2019a,
  ids = {steyerberg2009,steyerberg2009a},
  title = {Clinical {{Prediction Models}}: {{A Practical Approach}} to {{Development}}, {{Validation}}, and {{Updating}}},
  shorttitle = {Clinical {{Prediction Models}}},
  author = {Steyerberg, Ewout W.},
  year = {2019},
  series = {Statistics for {{Biology}} and {{Health}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-16399-0},
  urldate = {2021-10-21},
  isbn = {978-3-030-16398-3 978-3-030-16399-0},
  langid = {english},
  file = {C:\Users\4216318\Zotero\storage\CN5KNYXP\Steyerberg - 2019 - Clinical Prediction Models A Practical Approach t.pdf}
}

@article{therneau,
  title = {An {{Introduction}} to {{Recursive Partitioning Using}} the {{RPART Routines}}},
  author = {Therneau, Terry M and Atkinson, Elizabeth J and Foundation, Mayo},
  pages = {60},
  langid = {english},
  file = {C:\Users\4216318\Zotero\storage\J52CRYGH\Therneau et al. - An Introduction to Recursive Partitioning Using th.pdf}
}

@article{twala2009b,
  ids = {twala2009,twala2009a},
  title = {An {{Empirical Comparison}} of {{Techniques}} for {{Handling Incomplete Data Using Decision Trees}}},
  author = {Twala, Bhekisipho},
  year = {2009},
  month = may,
  journal = {Applied Artificial Intelligence},
  volume = {23},
  number = {5},
  pages = {373--405},
  publisher = {{Taylor \& Francis}},
  issn = {0883-9514},
  doi = {10.1080/08839510902872223},
  urldate = {2021-10-21},
  abstract = {Increasing the awareness of how incomplete data affects learning and classification accuracy has led to increasing numbers of missing data techniques. This article investigates the robustness and accuracy of seven popular techniques for tolerating incomplete training and test data for different patterns of missing data\textemdash different proportions and mechanisms of missing data on resulting tree-based models. The seven missing data techniques were compared by artificially simulating different proportions, patterns, and mechanisms of missing data using 21 complete datasets (i.e., with no missing values) obtained from the University of California, Irvine repository of machine-learning databases (Blake and Merz, 1998). A four-way repeated measures design was employed to analyze the data. The simulation results suggest important differences. All methods have their strengths and weaknesses. However, listwise deletion is substantially inferior to the other six techniques, while multiple imputation, that utilizes the expectation maximization algorithm, represents a superior approach to handling incomplete data. Decision tree single imputation and surrogate variables splitting are more severely impacted by missing values distributed among all attributes compared to when they are only on a single attribute. Otherwise, the imputation\textemdash versus model-based imputation procedures gave\textemdash reasonably good results although some discrepancies remained. Different techniques for addressing missing values when using decision trees can give substantially diverse results, and must be carefully considered to protect against biases and spurious findings. Multiple imputation should always be used, especially if the data contain many missing values. If few values are missing, any of the missing data techniques might be considered. The choice of technique should be guided by the proportion, pattern, and mechanisms of missing data, especially the latter two. However, the use of older techniques like listwise deletion and mean or mode single imputation is no longer justifiable given the accessibility and ease of use of more advanced techniques, such as multiple imputation and supervised learning imputation.},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\GLV5NMCB\\Twala - 2009 - An Empirical Comparison of Techniques for Handling.pdf;C\:\\Users\\4216318\\Zotero\\storage\\S3QFMVKI\\08839510902872223.html}
}

@article{uddin2019,
  ids = {uddin2019a},
  title = {Comparing Different Supervised Machine Learning Algorithms for Disease Prediction},
  author = {Uddin, Shahadat and Khan, Arif and Hossain, Md Ekramul and Moni, Mohammad Ali},
  year = {2019},
  month = dec,
  journal = {BMC Medical Informatics and Decision Making},
  volume = {19},
  number = {1},
  pages = {281},
  issn = {1472-6947},
  doi = {10.1186/s12911-019-1004-8},
  abstract = {Background: Supervised machine learning algorithms have been a dominant method in the data mining field. Disease prediction using health data has recently shown a potential application area for these methods. This study ai7ms to identify the key trends among different types of supervised machine learning algorithms, and their performance and usage for disease risk prediction. Methods: In this study, extensive research efforts were made to identify those studies that applied more than one supervised machine learning algorithm on single disease prediction. Two databases (i.e., Scopus and PubMed) were searched for different types of search items. Thus, we selected 48 articles in total for the comparison among variants supervised machine learning algorithms for disease prediction. Results: We found that the Support Vector Machine (SVM) algorithm is applied most frequently (in 29 studies) followed by the Na\"ive Bayes algorithm (in 23 studies). However, the Random Forest (RF) algorithm showed superior accuracy comparatively. Of the 17 studies where it was applied, RF showed the highest accuracy in 9 of them, i.e., 53\%. This was followed by SVM which topped in 41\% of the studies it was considered. Conclusion: This study provides a wide overview of the relative performance of different variants of supervised machine learning algorithms for disease prediction. This important information of relative performance can be used to aid researchers in the selection of an appropriate supervised machine learning algorithm for their studies.},
  langid = {english}
}

@article{vancalster2019,
  ids = {vancalsterCalibrationAchillesHeel2019a},
  title = {Calibration: The {{Achilles}} Heel of Predictive Analytics},
  shorttitle = {Calibration},
  author = {Van Calster, Ben and McLernon, David J. and {van Smeden}, Maarten and Wynants, Laure and Steyerberg, Ewout W. and Bossuyt, Patrick and Collins, Gary S. and Macaskill, Petra and McLernon, David J. and Moons, Karel G. M. and Steyerberg, Ewout W. and Van~Calster, Ben and {van~Smeden}, Maarten and Vickers, Andrew~J. and {On behalf of Topic Group `Evaluating diagnostic tests and prediction models' of the STRATOS initiative}},
  year = {2019},
  month = dec,
  journal = {BMC Medicine},
  volume = {17},
  number = {1},
  pages = {230},
  issn = {1741-7015},
  doi = {10.1186/s12916-019-1466-7},
  urldate = {2021-07-19},
  abstract = {The assessment of calibration performance of risk prediction models based on regression or more flexible machine learning algorithms receives little attention.},
  keywords = {Calibration,Heterogeneity,Model performance,Overfitting,Predictive analytics,Risk prediction models},
  file = {C\:\\Users\\4216318\\Zotero\\storage\\778ERK9Z\\Van Calster et al. - 2019 - Calibration the Achilles heel of predictive analy.pdf;C\:\\Users\\4216318\\Zotero\\storage\\MKIVQWCS\\Van Calster et al. - 2019 - Calibration the Achilles heel of predictive analy.pdf;C\:\\Users\\4216318\\Zotero\\storage\\FNGA3XYT\\s12916-019-1466-7.html;C\:\\Users\\4216318\\Zotero\\storage\\QJLCN2YI\\s12916-019-1466-7.html}
}

@article{vancalster2020,
  title = {Regression Shrinkage Methods for Clinical Prediction Models Do Not Guarantee Improved Performance: {{Simulation}} Study},
  shorttitle = {Regression Shrinkage Methods for Clinical Prediction Models Do Not Guarantee Improved Performance},
  author = {Van Calster, Ben and {van Smeden}, Maarten and De Cock, Bavo and Steyerberg, Ewout W},
  year = {2020},
  month = nov,
  journal = {Statistical Methods in Medical Research},
  volume = {29},
  number = {11},
  pages = {3166--3178},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0962-2802},
  doi = {10.1177/0962280220921415},
  urldate = {2021-08-18},
  abstract = {When developing risk prediction models on datasets with limited sample size, shrinkage methods are recommended. Earlier studies showed that shrinkage results in better predictive performance on average. This simulation study aimed to investigate the variability of regression shrinkage on predictive performance for a binary outcome. We compared standard maximum likelihood with the following shrinkage methods: uniform shrinkage (likelihood-based and bootstrap-based), penalized maximum likelihood (ridge) methods, LASSO logistic regression, adaptive LASSO, and Firth's correction. In the simulation study, we varied the number of predictors and their strength, the correlation between predictors, the event rate of the outcome, and the events per variable. In terms of results, we focused on the calibration slope. The slope indicates whether risk predictions are too extreme (slope\,{$<$}\,1) or not extreme enough (slope\,{$>$}\,1). The results can be summarized into three main findings. First, shrinkage improved calibration slopes on average. Second, the between-sample variability of calibration slopes was often increased relative to maximum likelihood. In contrast to other shrinkage approaches, Firth's correction had a small shrinkage effect but showed low variability. Third, the correlation between the estimated shrinkage and the optimal shrinkage to remove overfitting was typically negative, with Firth's correction as the exception. We conclude that, despite improved performance on average, shrinkage often worked poorly in individual datasets, in particular when it was most needed. The results imply that shrinkage methods do not solve problems associated with small sample size or low number of events per variable.},
  langid = {english},
  keywords = {Clinical risk prediction models,Firth's correction,logistic regression,maximum likelihood,penalized likelihood,shrinkage},
  file = {C:\Users\4216318\Zotero\storage\NM3IWVC5\Van Calster et al. - 2020 - Regression shrinkage methods for clinical predicti.pdf}
}

@article{vansmeden2019,
  ids = {vansmeden2019a},
  title = {Sample Size for Binary Logistic Prediction Models: {{Beyond}} Events per Variable Criteria},
  author = {{van Smeden}, Maarten and Moons, Karel GM and {de Groot}, Joris AH and Collins, Gary S and Altman, Douglas G and Eijkemans, Marinus JC and Reitsma, Johannes B},
  year = {2019},
  month = aug,
  journal = {Statistical Methods in Medical Research},
  volume = {28},
  number = {8},
  pages = {2455--2474},
  issn = {0962-2802, 1477-0334},
  doi = {10.1177/0962280218784726},
  abstract = {Binary logistic regression is one of the most frequently applied statistical approaches for developing clinical prediction models. Developers of such models often rely on an Events Per Variable criterion (EPV), notably EPV !10, to determine the minimal sample size required and the maximum number of candidate predictors that can be examined. We present an extensive simulation study in which we studied the influence of EPV, events fraction, number of candidate predictors, the correlations and distributions of candidate predictor variables, area under the ROC curve, and predictor effects on out-ofsample predictive performance of prediction models. The out-of-sample performance (calibration, discrimination and probability prediction error) of developed prediction models was studied before and after regression shrinkage and variable selection. The results indicate that EPV does not have a strong relation with metrics of predictive performance, and is not an appropriate criterion for (binary) prediction model development studies. We show that out-of-sample predictive performance can better be approximated by considering the number of predictors, the total sample size and the events fraction. We propose that the development of new sample size criteria for prediction models should be based on these three parameters, and provide suggestions for improving sample size determination.},
  langid = {english}
}

@article{vries2018,
  ids = {vries2018a},
  title = {Propensity {{Score Estimation Using Classification}} and {{Regression Trees}} in the {{Presence}} of {{Missing Covariate Data}}},
  author = {Vries, de Bas B. L. Penning and Smeden, van Maarten and Groenwold, Rolf H. H.},
  year = {2018},
  journal = {Epidemiologic Methods},
  volume = {7},
  number = {1},
  pages = {20170020},
  doi = {10.1515/em-2017-0020}
}

@book{zotero-6275,
  title = {2.3 {{Comparison}} of Performance of Data Imputation Methods in the Context of Their Impact on the Prediction Efficiency of Classification Algorithms | {{ML Case Studies}}},
  urldate = {2021-06-23},
  abstract = {Case studies for reproducibility, imputation, and interpretability},
  file = {C:\Users\4216318\Zotero\storage\ERF5RT9V\comparison-of-performance-of-data-imputation-methods-in-the-context-of-their-impact-on-the-pred.html}
}

@misc{zotero-6517,
  title = {Evaluating a Logistic Regression Based Prediction Tool in {{R}} |},
  urldate = {2021-07-19},
  howpublished = {https://darrendahly.github.io/post/homr/}
}
