---
title: "SIG Project Real-Time Imputation"
author: "Steven Nijman*, Hanne Oberman*, Gerko Vink, Thomas Debray, Maarten van Smeden"
output:
  html_document:
    df_print: paged
    toc: true
    number_sections: true
---

Simulation pipeline organized according to the ADEMP approach: Aims, Data-generating mechanisms, Methods, Estimands, Performance measures.

TODO:

- Check where else it says "TODO" and "Add" throughout this file

- check c-stat [.6-.9]

- report the betas

- Add motivating example with MIMIC data (first impute once with some non-parametric model like an AE, then fit some prediction model on the completed population data)

- Choose a nice logistic prediction model for the motivating example, like a certain diagnosis or 48h mortality (not 30 day mortality, because then you need time-to-event models)

- Rename mean imp to CMI

- Rerun sim without higher order terms    

- After running the simulation, check convergence of the population diagnostics to see if 1000 repetitions was enough

- TODO: add practical implication of imputation methods to discussion -> imputed values predictors may be clinically relevant.


![](./www/simulationdiagram.png)


```{r setup, message = FALSE, warning = FALSE, include = FALSE}
# packages
library("dplyr") #for wrangling
library("purrr") #for vectorization
library("mvtnorm") #for DGM
library("mice") #for DGM
library("rms") #for prediction
library("party") #for prediction
library("ranger") #for prediction

# functions 
source("./R/setup.R")

# parameters
set.seed(11)
n_sim <- 2 #TODO: make this 1000
n_devset <- 10000 #TODO: make this 10000
n_valset <- 20000 #TODO: make this 20000
m <- 11 #TODO: make this 51
p <- 10 #p_missing is hard coded as 4, 6 or 8
DGM <- define_DGM(p)
```

# Aims

This document contain the set-up of our SIG project titled "An evaluation of 'real-time' missing data handling in machine learning and prevailing statistical models". The aim is to compare different strategies for developing prediction models that can handle the presence of missing values real time in a single patient. Preferred journal: Stat. Med.

# Data-Generating Mechanisms

We use model-based simulation, with a single data-generating mechanism for the development data and validation sets. The development set is used as-is; the validation set is subsequently 'amputed'.

## DGM model

We use a DGM model with: 

- 10 continuous predictors ($X_1$, $X_2$, ..., $X_{10}$), with cases generated from a multivariate normal distribution;^[Note. Potentially add dichotomous predictor(s) later.]
- 1 binary outcome ($Y$), calculated from the 10 predictors.

We decided against the use of a 'real world' variance-covariance matrix for the predictor variables. Initially, we were going to use the SMART data as the basis for our predictor space. But this would result in the same limitations as described in Nijman et al. (2021; i.e., low correlations between predictor variables). 

Therefore, we randomly generated a variance-covariance matrix for the purpose of this study. This matrix^[Note. Maybe add more variance-covariance matrices for sensitivity analyses later. E.g., add one with 200 predictors.] serves as the basis for each development and/or validation dataset used in this study. 

```{r varcov}
# visualize variance-covariance matrix with p predictors
ggcorrplot::ggcorrplot(cov2cor(DGM$varcov), type = "lower", lab = TRUE)

# print vectors with regression coefficients for linear and non-linear effects
(linear_effects <- DGM$betas[1:10])
(non-linear_effects <- DGM$betas[11:20])
```

We use the 10 predictors to calculate the dichotomous outcome. The current DGM model includes one cubic term and 9 interactions. For additional complexity, we add one log-transformation (the natural logarithm of the absolute value of the second predictor).

Regression coefficients for the linear predictor of Y are `r DGM$betas[1:p]` for the univariate effects of the 10 predictors. Regression coefficients for the interaction effects are `r DGM$betas[(p+1):(2*p)]`. 

## Development set

We use a sample size of 10.000 in the development sets. One such set is generated below.

```{r devset}
# let's generate some data 
devset <-
  generate_data(
    sample_size = n_devset,
    covariance_matrix = DGM$varcov,
    linear_bs = DGM$betas[1:p],
    non_linear_bs = DGM$betas[(p+1):(2*p)],
    interaction = TRUE
  )

# what do the data look like?
glimpse(devset)
ggcorrplot::ggcorrplot(cor(devset[, -1]), type = "lower", lab = TRUE, digits = 3, legend.title = "Correlation")

```

The aim is to have a prevalence of about 0.15. In this dev set, the prevalence of the outcome is `r mean(devset$Y)`. We can now fit the different models.

```{r fit}
# fitt all models
mod <- fit_mod(devset)

# check auc of logistic model
auc_log <-
    pROC::roc(
      Y ~ prob,
      data = cbind(devset, prob = predict(mod$log, newdata = devset[,-1])),
      quiet = TRUE
    )$auc
```

The c-statistics of the logistic model is `r auc_log`.

## Validation set

We use a sample size of 20000 in the validation sets, to make sure there are at least 2000 events in de val set, with the prevalence of ~0.15. 

```{r valset}
# generate complete validation set
valset <-
  generate_data(
    sample_size = n_valset,
    DGM$varcov,
    DGM$betas[1:p],
    DGM$betas[(p+1):(2*p)],
    interaction = TRUE
  )
```

The number of events in this val set is `r sum(valset$Y)`. The next step is to ampute the validation set using several missing data patterns and missingness mechanisms. 

We use a mixture of three missing data patterns, with 40%, 60%, and 80% of variables being missing per case. The missingness mechanisms are a mixture as well: we combine the four types of MAR missingness ("MAR left", "MAR right", "MAR mid", and "MAR tail").

```{r missingness}
# check the MAR missingness mechanisms
DGM$miss_type
```

Ampute the validation set according to the missing data patterns and missingness mechanisms.

```{r ampute}
# make the validation set incomplete according to the missing data pattern and MAR types
valset <- create_miss(valset, missingness_pat = DGM$miss_pat, missingness_type = DGM$miss_type)
# check missing data pattern
md <- mice::md.pattern(valset[,-c(1:3)])
```

# Estimands

We calculate the absolute outcome risk for each individual observation (i.e. patient) according to five different strategies for dealing with missing values, and three different prediction models.


# Methods

The three methods for prediction model development are:

- Strategy 1: The prediction model is a flexible regression model including non-linear effects using RCS (restricted cubic splines) with 3 knots^[check definition, use package `RCS`]. We also store the means and covariance of all predictor variables (using package `condmvnorm`, which can be used to generate imputations).

- Strategy 2: The prediction model is a "box" of submodels: a flexible  regression model is developed for each possible combination of available predictors. If we have 10 predictor variables, this means that we would have to fit 1  + 10 + 45 + 120 + 210 + 252 + 252 + 210 + 120 + 45  + 10 + 1  = 1276 regression models. However, there is no need to estimate all these models. We can first look in the hold-out sample what variable is missing, and then estimate the necessary "submodel". In our case, we estimate just 3 submodels (with 2, 4, or 6 variables observed according to the missing data pattern).

- Strategy 3: The prediction model is a random forest, as implemented by `cforest()` in the R package `party`. We generate a certain number of surrogate splits for each node. These splits attempt to mimic the primary split, and thus to achieve similar separation using another (observed) variable. By default set 4 surrogate splits (since we set max 3 missings). See "surrogate decisions based on additional variables (cf. Breiman et al., 1984; Hothorn et al., 2006)" (Hapfelmeier, p. 6). This is **very** computationally inefficient, so we'll need HPC power. Because in the clinical setting a 15 minute fitting procedure is not terrible, we do not scrap this method. However, we could also implement random forests in the presence of missing data using the `missranger` package. We'll add this method because it seems popular and efficient. We'll also use the joint method (Nijman et al.) to impute before fitting the `ranger` random forest.

Sidenote for strategy 3: Random forests are very susceptible to variations in the calibration slope, even in contexts without missingness. With huge devsets (i.e., `n_obs > 1.000.000`) or average performance across simulation repetitions, this problem fades. But we'll need to specifically check the calibration of the random forest models just to be sure!

The five methods for dealing with missing values are:

- (Only for strategy 1 and 3): Missing values are imputed by their conditional mean.^[What to do with categorical predictors? Make dummy variables for each level? See Nijman et al paper!]

- (Only for strategy 1 and 3): Missing values are imputed by a random draw from their conditional multivariate distribution.

- (Only for strategy 1 and 3): Missing values are imputed 50 times by a random draw from their distribution. The resulting 50 absolute risk predictions are then averaged to obtain the final prediction.

- (Only for strategy 2): The appropriate pattern submodel is selected for calculating an absolute risk.

- (Only for strategy 3): Missing values are handled using the surrogate splits.

For the first three missing data methods, we impute the missing values first, the pattern submodel and surrogate split methods do not require this extra step. 

Let's generate predictions for each of the methods (note that the imputation step happens within this function).

```{r predict}
predictions <- pred_outcome(validation_set = valset, fitted_mod = mod, n_imp = m) 
```
Why do we get these error messages?

# Performance measures

List all performance measures to be estimated, justifying their relevance to estimands or other targets. 

We evaluate the predicted outcome risk against the original outcome risk (i.e. the absolute outcome risk that would be obtained if we have no missing values). 
Calculate:

- Root mean square prediction error;

- Brier score (predicted risk vs observed outcome).

Finally, across all hold-out patients:

-	Calibration of (predicted Y | one or more missing predictor values) versus (predicted Y | original predictor values) --> calibration slope (voor methode 1 underfitting and slope > 1?);

-	Discrimination of (predicted Y | one or more missing predictor values) versus (predicted Y | original predictor values);

-	Visual inspection of calibration plot (see plot Gary Collins in Stat med paper, min 100 events).

TODO: Add **MAPE, RMSPE, mean absolute distance wrt real prob., think about AIC and calibration intercept and calibration slope on population level while we're interested in individual level.**

TODO: add real intercept (this one is re-estimated) using logistic model (en met offset eromheen voor intercept).

root mean sq distance van log calib slope (ideaal is 0, dus verder is slechter). bijv van lasso hele hoge variantie terwijl gem perfect is.

```{r performance}
# put the predictions in a list to suit the old code below
pred_list <- list(Y_prob = predictions$Y_prob,
                  Y_obs = predictions$Y_true,
                  Y_sub = predictions$Y_pred_sub,
                  Y_rf_sur = predictions$Y_pred_sur,
                  Y_mean_log = predictions$Y_pred_log_mean,
                  Y_sing_log = predictions$Y_pred_log_sing,
                  Y_mult_log = predictions$Y_pred_log_mult,
                  Y_mean_rf = predictions$Y_pred_rf_mean,
                  Y_sing_rf = predictions$Y_pred_rf_sing,
                  Y_mult_rf = predictions$Y_pred_rf_mult)
# RMSE
source("./R/perf.r")
(RMSE_obs <- purrr::map(pred_list, ~ calc_RMSE(pred = ., obs = pred_list[["Y_obs"]])))
(RMSE_prob <- purrr::map(pred_list, ~ calc_RMSE(pred = ., obs = pred_list[["Y_prob"]])))

# brier score
# (brier_obs <- purrr::map(pred_list, ~ calc_brier(pred = ., obs = valset[, "Y_obs"])))
(brier_prob <- purrr::map(pred_list, ~ calc_brier(pred = ., obs = pred_list[["Y_prob"]])))
# Let op: Komt de kans richting het gemiddelde? Zie prestatie van 'methode' met alleen 0en als predicted prob invullen.
# purrr::map(predictions, ~{rms::val.prob(p = predictions$.x, y = predictions$Y_true)})
rms::val.prob(p = predictions$Y_pred_sub, y = predictions$Y_true)
rms::val.prob(p = predictions$Y_pred_log_mult, y = predictions$Y_true)

# plot calibration
plotdat <- data.frame(obs = rep(pred_list$Y_prob,4), pred = c(pred_list$Y_sub, pred_list$Y_imp_mean, pred_list$Y_imp_sing, pred_list$Y_imp_mult), method = c("submodel", "cond.imp", "sing.imp", "mult.imp"))
ggplot(plotdat, aes(x = pred, y = obs, color = method)) +
  geom_point(alpha = 0.01) +
  geom_smooth(fullrange = T) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") + 
  facet_wrap(~method) + 
  theme_classic() + 
  lims(x = c(0,1), y = c(0,1))
```

