---
title: "SIG Project Real-Time Imputation"
author: "Steven Nijman, Thomas Debray, Maarten van Smeden, Gerko Vink, Hanne Oberman"
output:
  html_document:
    df_print: paged
    toc: true
    number_sections: true
---

Simulation pipeline organized according to the ADEMP approach: Aims, Data-generating mechanisms, Methods, Estimands, Performance measures.

![](./www/simulationdiagram.png)


```{r setup, message = FALSE, warning = FALSE, include = FALSE}
# environment
library("dplyr") #for the pipe operator
library("ggcorrplot") #for easy visualization of correlations
library("mvtnorm") #for multivariate normal distributions
library("pROC") #for easy calculation of the auc
library("mice") #for missing data stuffs
library("ranger")
library("randomForest")

# # data
# load("./Data/varcov.RData") #variance-covariance matrix of the SMART dataset

# functions
source("./R/DGM.R") #data generating mechanism
```

# Aims

This document contain the set-up of our SIG project titled "An evaluation of 'real-time' missing data handling in machine learning and prevailing statistical models". The aim is to compare different strategies for developing prediction models that can handle the presence of missing values real time in a single patient.



# Data-Generating Mechanisms

We use design-based simulation, with a single data-generating mechanism for the development data and validation sets. The development set is used as-is; the validation set is subsequently 'amputed'.

## DGM model

We use a DGM model with: 

- 10 continuous predictors ($X_1$, $X_2$, ..., $X_{10}$), with cases generated from a multivariate normal distribution;^[Note. Potentially add dichotomous predictor(s) later.]
- 1 binary outcome ($Y$), calculated from the 10 predictors.

We decided against the use of a 'real world' variance-covariance matrix for the predictor variables. Initially, we were going to use the SMART data as the basis for our predictor space. But this would result in the same limitations as described in Nijman et al. (2021; i.e., low correlations between predictor variables). 

Therefore, we randomly generated a variance-covariance matrix for the purpose of this study. This matrix^[Note. Maybe add more variance-covariance matrices for sensitivity analyses later.] serves as the basis for each development and/or validation dataset used in this study. 

```{r varcov}
# create a variance-covariance matrix with p predictors
set.seed(123)
p <- 10
betas <- rnorm(p * (p - 1) / 2, 0, 0.1)
sigma <- diag(p)
sigma[upper.tri(sigma)] <- betas
sigma[lower.tri(sigma)]  <- t(sigma)[lower.tri(sigma)]
Sigma <- t(sigma) %*% sigma
# isSymmetric(Sigma)
ggcorrplot::ggcorrplot(cov2cor(Sigma), type = "lower", lab = TRUE)
```

We use the 10 predictors to calculate the dichotomous outcome. The current DGM model includes one cubic term and 9 interactions. For additional complexity, we add one log-transformation (the natural logarithm of the absolute value of the second predictor).


## Development set

We use a sample size of 10.000 in the development sets. One such set is generated below.

```{r devset}
# let's generate some data 
n <- 10000
set.seed(11)

# generate the regression coefficients
linear_betas <- runif(nrow(Sigma),-1, 1)
non_linear_betas <- runif(10,-0.1, 0.1)
dat <-
  generate_sample(n, Sigma, linear_betas, non_linear_betas, interaction = TRUE)

# what do the data look like?
glimpse(dat)

# save data
saveRDS(dat, file = "Data/dataset.RDS")
```

The aim is to have a prevalence of about 0.15. In this dev set, the prevalence of the outcome is `r mean(dat$Y)`. 

How well do the three types of models fit?

```{r fit}
# check prevalence and auc of the models
# logistic model
logistic <- fit_logistic(dat)
# saveRDS(logistic, file = "Data/logistic_model.RDS")

# pattern mixture models
mod6 <- glm(Y ~ X1 + X2 + X3 + X4 + X5 + X6, family = "binomial", data = dat)
mod4 <- glm(Y ~ X1 + X2 + X3 + X4, family = "binomial", data = dat)
mod2 <- glm(Y ~ X1 + X2, family = "binomial", data = dat)
auc6 <-
  pROC::roc(Y ~ prob,
            data = cbind(dat, prob = predict(mod6)),
            quiet = TRUE)$auc
auc4 <-
  pROC::roc(Y ~ prob,
            data = cbind(dat, prob = predict(mod4)),
            quiet = TRUE)$auc
auc2 <-
  pROC::roc(Y ~ prob,
            data = cbind(dat, prob = predict(mod2)),
            quiet = TRUE)$auc

# random forest
rf <- fit_rf(dat)
rp <- rpart::rpart(Y~.,dat)
# plot(rp)
# text(rp)
# saveRDS(rf, file = "Data/rf_model.RDS")
```

Run random forest in party package

```{r}
# start <- Sys.time()
rf_party <- party::cforest(Y~., data=dat)
# end <- Sys.time()
# (elapsed <- end-start)
```

The C index/AUC of the logistic model (without interactions) is `r logistic$auc`, of the submodels it's `r round(auc6,2)`, `r round(auc4,2)`, and `r round(auc2,2)`, and of the random forest `r rf$auc`. This perfect accuracy of the rf on the training set should not be a problem, see https://towardsdatascience.com/one-common-misconception-about-random-forest-and-overfitting-47cae2e2c23b. But just to check, we fit these models on a new dev set.

```{r}
# why is the auc of the rf 1? test all models on new data
devset <-
  generate_sample(n, Sigma, linear_betas, non_linear_betas, interaction = TRUE)

# logistics model
auc_logistic <-
    pROC::roc(
      Y ~ prob,
      data = cbind(devset, prob = predict(logistic$mod, newdata = devset)),
      quiet = TRUE
    )$auc
# pattern mixture models
auc_pm6 <-
    pROC::roc(
      Y ~ prob,
      data = cbind(devset, prob = predict(mod6, newdata =  devset)),
      quiet = TRUE
    )$auc
auc_pm4 <-
    pROC::roc(
      Y ~ prob,
      data = cbind(devset, prob = predict(mod4, newdata =  devset)),
      quiet = TRUE
    )$auc
auc_pm2 <-
    pROC::roc(
      Y ~ prob,
      data = cbind(devset, prob = predict(mod2, newdata =  devset)),
      quiet = TRUE
    )$auc
# rf model
pred_rf <-
  predict(rf$mod, data = devset) %>% .$predictions %>% cbind(devset, prob = .)
auc_rf <- pROC::roc(Y ~ prob,
                     data = pred_rf,
                     quiet = TRUE)$auc
pred_rp <- predict(rp, newdata = devset)
auc_rp <- pROC::roc(Y ~ prob,
                     data = cbind(devset, prob = pred_rp),
                     quiet = TRUE)$auc
```
Check with party package

```{r}
# start <- Sys.time()
# pred_party <- predict(rf_party, newdata = devset)
# end <- Sys.time()
# (elapsed2 <- end-start)
```



The AUC in the new data is:

- `r auc_logistic` for the logistic model;

- `r auc_pm6`, `r auc_pm4`, and `r auc_pm2` for the pattern mixture models;

- `r auc_rf` for the random forest.
 

## Validation set

We use a sample size of 20000 in the validation sets, to make sure there are at least 2000 events in de val set, with the prevalence of ~0.15. 

<!-- ### Q: I wrote down that we need 200 events in the val set, but also that we need 2000 events in the val set. Which one is correct? More is better, so at least 200 but rather  2000. -->


```{r valset}
valset <- generate_sample(sample_size = 20000, Sigma, linear_betas, non_linear_betas, interaction = TRUE)

# # now redundant: what is the strongest predictor?
# dat %>% 
#   glm(Y ~ ., family = "binomial", data = .) %>% 
#   broom::tidy() %>% 
#   dplyr::arrange(desc(abs(estimate)))

```

The number of events in this val set is `r sum(valset$Y)`. The next step is to ampute the validation set using several missing data patterns and missingness mechanisms. 

We use a mixture of three missing data patterns, with 40%, 60%, and 80% of variables being missing per case. The missingness mechanisms are a mixture as well: we combine the four types of MAR missingness ("MAR left", "MAR right", "MAR mid", and "MAR tail").

```{r missingness}
# define the types of MAR missngness
MAR_types <- rep(c("LEFT", "MID", "TAIL", "RIGHT"), 3) 
# set types of MAR missingness outside of simulation loop
# set.seed(1)
# sample(c("LEFT", "MID", "TAIL", "RIGHT"), size = 12, replace = TRUE)

# create missing data patterns
pat <-  matrix(1, 12, 10) %>% 
  data.frame() 
# four var missing
pat[1:4,7:10] <- 0 
# six var missing
pat[5:8,5:10] <- 0 
# eight var missing
pat[9:12,3:10] <- 0
```

Ampute the validation set according to the missing data patterns and missingness mechanisms.

```{r ampute}
# make the validation set incomplete according to the missing data pattern and MAR types
testset <- valset[,-1] %>% 
  mice::ampute(mech = "MAR", prop = 0.999, patterns = pat, type = MAR_types) %>% 
  .$amp 
md <- mice::md.pattern(testset)
```


<!-- ### Q: The ampute function allows for different types of MAR across patterns, not variables. But last meeting we discussed "mechnismes mixen over kolommen 1 kolom is dan bijv. altijd MAR right". Is this a problem? -->

<!-- ### Q: Just to check: currently we only introduce missingness in 'simple' predictors (vs 'complex', e.g. quadratic/transformed), is that ok? -->


<!-- ## Discussion topics: DGM -->

<!-- - Alle modellen alleen op complete data fitten (RF, log reg, Pattern mixture submodel)  -->

<!-- - Dataset voor ontwikkelen predictiemodel regressie, imputatie, en rf -->

<!-- - En een dataset om te imputeren, waarin missing values moeten komen -->

<!-- - Onder hetzelfde model genereren we nieuwe data voor het valideren -->

<!-- - Idealiter zouden de regressie en rf modellen op de complete data dezelfde c-index/prestatie hebben -->

<!-- - En dan trekken we steeds een validatieset op basis van hetzelfde mechanisme en daar schieten we gaten in volgens de missingness mechanisms -->

<!-- - Calibratieslope varieert enorm: onder en overfitting wisselt elkaar af, dus je kunt pech of geluk hebben. Bij een dev set van 1 miljoen dan heeft ie perfecte calibratie. In de gemiddelde performance verdwijnt het, maar wat bij rf steeds misgaat is de variantie in de prestatie. Als je een model ontwikkelt, zit je soms heel ver van een goed model af. -->

<!-- - Conclusie: zowel dev als val set trekken we meermaals. Voorafgaand aan de analyse pipeline. n_val = 500? -->


<!-- We'll start with a missing data pattern where joint missingness occurs in 3 variables. 50-75% of the rows has at least one of the sets of 3 variables missing. -->

<!-- In the original proposal, the missing data mechanisms were defined as: -->

<!-- -	MCAR strategy: for all hold-out patients, the same variable(s) are missing. We will set probability of missing to less than 100%, e.g. 50%. -->

<!-- -	MAR strategy: different variables are missing across the hold-out patients. The probability of missingness depends on some of the observed patient characteristics. Interpretation of RMSE now depends on missing data pattern, and is therefore more complex. E.g., use AMPUTE. Maximal 3 missing jointly (mechanism depends on the other 7 vars). 1 situation with 1 fixed var missing, 1 situation with 2 fixed var missing.  -->

<!-- -	MNAR strategy: variables are missing according to their original value. Do it for 1 predictor, the strongest one. -->

<!-- ### Can we just apply the same pattern to each of the mechanisms? -->

<!-- ### Q: Should the missing data pattern be equal for all three missing data mechanisms? -->

<!-- A: Yes. Some variables are missing more often than others. For example, lab measures are missing more often. We'll create 3 or 4 patterns where joint missingness occurs in 3 variables. 50-75% of the rows has at least one of the sets of 3 variables missing. -->

<!-- ### Q: Should the outcome variable contain missingness? It won't be observed anyways, right? -->
<!--  A: Indeed, it will be predicted. -->

<!-- ### Q: Should we amupte each row once? To ammulate a 'real' situation? Or should we apply each mechanism to each case.  -->

<!-- A: Each mechanism gets its own amputed dataset. To get equivalent problems for each method. -->


<!-- Discussion topics: Missingness conditions -->

<!-- - Missingness mechanisms: de val set incompleet maken adhv MAR (want MCAR is niet realistisch, en MNAR is te ingewikkeld zonder de MAR performance te weten) -> mixture van verschillende MAR patronen -> mixen over kolommen: een kolom is altijd MAR right -->

<!-- - Proportion of incomplete cases: voor alle individuen in de praktijk ontbreekt er iets. -->

<!-- - Number of predictors with missingness: 4, 6 of 8 van de 10 missend binnen 1 set -->



<!-- ### Does it matter if we develop the logistic regression prediction model on the complete dataset or the imputed incomplete set? -->

<!-- What we do know: -->

<!-- - The training data comes from the same population as the hold out set. -->

<!-- - We compare the methods at the prediction level.  -->


<!-- What we don't know: -->

<!-- - Would we need 4 datasets then? -->

<!-- - Does that mean that we fit 4*3 prediction models? -->

<!-- - Should we create 2 missing data patterns (i.e., just missingness in the 'regular' predictors vs. in the non-linear predictors)? -->

# Estimands

Define estimands and/or other targets of the simulation study: 

- We calculate the absolute outcome risk according to different strategies for dealing with missing values.



# Methods

Identify methods to be evaluated and consider whether they are appropriate for estimand/target identified. 

Methods for prediction model development:

- Strategy 1: The prediction model is a flexible regression model including non-linear effects using RCS (restricted cubic splines) with 3 knots. We also store the means and covariance of all predictor variables (which can be used to generate imputations). package condmvnorm.

- Strategy 2: The prediction model is a "box" of submodels: a flexible  regression model is developed for each possible combination of available predictors. If we have 10 predictor variables, this means that we would have to fit 1  + 10 + 45 + 120 + 210 + 252 + 252 + 210 + 120 + 45  + 10 + 1  = 1276 regression models. However, there is no need to estimate all these models. We can first look in the hold-out sample what variable is missing, and then estimate the necessary "submodel". In our case, we estimate just 3 submodels (with 2, 4, or 6 variables observed according to the missing data pattern).

- Strategy 3: The prediction model is a random forest, as implemented by cforest() in the R package party. We generate a certain number of surrogate splits for each node. These splits attempt to mimic the primary split, and thus to achieve similar separation using another (observed) variable. By default set 4 surrogate splits (since we set max 3 missings). See "surrogate decisions based on additional variables (cf. Breiman et al., 1984; Hothorn et al., 2006)" (Hapfelmeier, p. 6). This is **very** computationally inefficient. Alternatively, we could implement random forests using the `missranger` package. 

- Toevoegen: RF icm de joint methode van steven

- Toevoegen: RF via missranger

- Niet toevoegen: predictor indicator methode 

HPC kan wel. 15 min voor real-time is niet problematisch.
Het is wel een logische methode om te gebruiken want we zijn bij predictie ook niet geinteresseerd in CIs. De ware kans boeit niet. MI is niet noodzakelijk, gepoolde schatting is wel iets stabieler. Maar calibratiecurve maken (LOESS) is wel ingewikkeld na imputatie.

Checken calibratie RF. Model tuning maakt heel erg uit. Zelfs zonder missingness. Bij gini moet je voorkomen. Niet tunen/default is gewoon gemiddelde. Met verkeerde instellingen is resultaat onpubliceerbaar.

Checken AUC tov regressie.



<!-- ### Q: Should we indeed include interaction terms and splines in the regression model like the original study proposal? Or should we leave then out to make sure the performance of the different models is comparable? Ok, we can leave them out of the prediction model. Or do both, to look at the diffs. Otherwise we have vanilla stats and advanced ML or the other way around. So incl RSC, but not interact. -->

<!-- ### Q: Is there an example/article/repo for the surrogate splits random forest model? This is already incl. in the cforest package. Hapmeier?? See email. Also ask Zoe Dunias about tuning of rf. -->

```{r}
# generate predictions for each of the cases in the (incomplete) testset
testset %>% glimpse() 

# start with easiest method: box of submodels
testset$nob <- rowSums(!is.na(testset)) #add variable for the number of observed variables
testsplit <- split(testset, testset$nob)
Y_pred_mod2 <- predict(mod2, newdata = testsplit$`2`)

# test with random forest
test <- predict(rf_party, newdata = testset)
Y_pred_rp <- predict(rp, newdata = testset)
```


Methods for generating absolute risk predictions:

- (Only for strategy 1 and 3): Missing values are imputed by their conditional mean => This strategy should work just fine.

- (Only for strategy 1 and 3): Missing values are imputed by a random draw from their conditional multivariate distribution

- (Only for strategy 1 and 3): Missing values are imputed 50 times by a random draw from their distribution. The resulting 50 absolute risk predictions are then averaged to obtain the final prediction.

- (Only for strategy 2): The appropriate pattern submodel is selected for calculating an absolute risk

- (Only for strategy 3): Missing values are handled using the surrogate splits


# Performance measures

List all performance measures to be estimated, justifying their relevance to estimands or other targets. 

We evaluate the predicted outcome risk against the original outcome risk (i.e. the absolute outcome risk that would be obtained if we have no missing values). Calculate:

- Root mean square prediction error;

- Brier score (predicted risk vs observed outcome).

Finally, across all hold-out patients:

-	Calibration of (predicted Y | one or more missing predictor values) versus (predicted Y | original predictor values);

-	Discrimination of (predicted Y | one or more missing predictor values) versus (predicted Y | original predictor values);

-	Visual inspection of calibration plot (see plot Gary Collins in Stat med paper min 100 events).



<!-- discr (c of auc) en calibr: interc en slope -->
<!-- mse van de predicties -->
<!-- verschil tussen voorspelde uitkomst en ware uitkomst (vanwege simulatie hier nu mogelijk) -->

<!-- sensitiviteit en specificiteit niet belangrijk hier -->




