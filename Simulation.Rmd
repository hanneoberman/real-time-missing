---
title: "SIG Project Real-Time Imputation"
author: "Steven Nijman*, Hanne Oberman*, Gerko Vink, Thomas Debray, Maarten van Smeden"
output:
  html_document:
    df_print: paged
    toc: true
    number_sections: true
---

Simulation pipeline organized according to the ADEMP approach: Aims, Data-generating mechanisms, Methods, Estimands, Performance measures.

![](./www/simulationdiagram.png)


```{r setup, message = FALSE, warning = FALSE, include = FALSE}
# environment
library("dplyr") #for the pipe operator
library("ggcorrplot") #for easy visualization of correlations
library("mvtnorm") #for multivariate normal distributions
library("purrr") #for vectorized 'for loops'
library("pROC") #for easy calculation of the auc
library("mice") #for missing data stuffs
library("ranger") #for fast random forests
library("missRanger") #for fast random forests with imputation
# library("party") #for random forests with surrogate splits
# library("moreparty") #for faster version of random forests with surrogate splits
# library("foreach") #for parallelization of random forests
# library("doParallel")
# doFuture::registerDoParallel()
# library("doFuture")
# doFuture::registerDoFuture()
# y <- plyr::llply(1:2, identity, .parallel = TRUE)
set.seed(1)

# # data
# load("./Data/varcov.RData") #variance-covariance matrix of the SMART dataset

# functions
source("./R/DGM.R") #data generating mechanism
source("./R/fit.R") #fit models on complete data
source("./R/pred.R") #predict outcome variable from incomplete data
source("./R/perf.R") #compute performance measures for each method
```

# Aims

This document contain the set-up of our SIG project titled "An evaluation of 'real-time' missing data handling in machine learning and prevailing statistical models". The aim is to compare different strategies for developing prediction models that can handle the presence of missing values real time in a single patient.

# Data-Generating Mechanisms

We use design-based simulation, with a single data-generating mechanism for the development data and validation sets. The development set is used as-is; the validation set is subsequently 'amputed'.

## DGM model

We use a DGM model with: 

- 10 continuous predictors ($X_1$, $X_2$, ..., $X_{10}$), with cases generated from a multivariate normal distribution;^[Note. Potentially add dichotomous predictor(s) later.]
- 1 binary outcome ($Y$), calculated from the 10 predictors.

We decided against the use of a 'real world' variance-covariance matrix for the predictor variables. Initially, we were going to use the SMART data as the basis for our predictor space. But this would result in the same limitations as described in Nijman et al. (2021; i.e., low correlations between predictor variables). 

Therefore, we randomly generated a variance-covariance matrix for the purpose of this study. This matrix^[Note. Maybe add more variance-covariance matrices for sensitivity analyses later. E.g., add one with 200 predictors.] serves as the basis for each development and/or validation dataset used in this study. 

```{r varcov}
# create a variance-covariance matrix with p predictors
varcov <- define_varcov(p = 10)
ggcorrplot::ggcorrplot(cov2cor(varcov), type = "lower", lab = TRUE)

# create vectors with regression coefficients for linear and non-linear effects
linear_betas <- runif(nrow(varcov),-1, 1)
non_linear_betas <- runif(nrow(varcov),-0.1, 0.1)
```

We use the 10 predictors to calculate the dichotomous outcome. The current DGM model includes one cubic term and 9 interactions. For additional complexity, we add one log-transformation (the natural logarithm of the absolute value of the second predictor).


## Development set

We use a sample size of 10.000 in the development sets. One such set is generated below.

```{r devset}
# let's generate some data 
devset <-
  generate_sample(sample_size = 10000, covariance_matrix = varcov, linear_bs = linear_betas, non_linear_bs = non_linear_betas, interaction = TRUE)

# what do the data look like?
glimpse(devset)
```

The aim is to have a prevalence of about 0.15. In this dev set, the prevalence of the outcome is `r mean(devset$Y)`. We can now fit the different models.


```{r fit}
# logistic model
mod_true <- fit_true(devset)

# box of submodels
mod_sub_6 <- fit_sub(devset, Y ~ X1 + X2 + X3 + X4 + X5 + X6) 
mod_sub_4 <- fit_sub(devset, Y ~ X1 + X2 + X3 + X4) 
mod_sub_2 <- fit_sub(devset, Y ~ X1 + X2) 

# random forest models with imputation and with surrogate splits
mod_rf_imp <- fit_rf_imp(devset)
mod_rf_sur <- fit_rf_sur(devset)
```

To see how well these models fit the complete data, we calculate the out of sample auc on some new data.

```{r auc, echo=FALSE, warning=FALSE}
# test all models on new data to compute the out of sample auc
aucset <-
  generate_sample(sample_size = 10000, varcov, linear_betas, non_linear_betas, interaction = TRUE)

# logistic model
auc_true <-
    pROC::roc(
      Y ~ prob,
      data = cbind(aucset, prob = predict(mod_true$mod, newdata = aucset[,-1])),
      quiet = TRUE
    )$auc
# box of submodels
auc_sub_2 <-
    pROC::roc(
      Y ~ prob,
      data = cbind(aucset, prob = predict(mod_sub_2$mod, newdata =  aucset[,-1])),
      quiet = TRUE
    )$auc
auc_sub_4 <-
    pROC::roc(
      Y ~ prob,
      data = cbind(aucset, prob = predict(mod_sub_4$mod, newdata =  aucset[,-1])),
      quiet = TRUE
    )$auc
auc_sub_6 <-
    pROC::roc(
      Y ~ prob,
      data = cbind(aucset, prob = predict(mod_sub_6$mod, newdata =  aucset[,-1])),
      quiet = TRUE
    )$auc
# rf models
auc_rf_imp <-
  pROC::roc(Y ~ prob,
            data = cbind(aucset, prob = predict(mod_rf_imp$mod, data = aucset[,-1])[["predictions"]]),
            quiet = TRUE)$auc
# pred_rf <-
#   predict(rf$mod, data = aucset) %>% .$predictions %>% cbind(aucset, prob = .)
# auc_rf <- pROC::roc(Y ~ prob,
#                      data = pred_rf,
#                      quiet = TRUE)$auc

```

The AUC in the new data is:

- `r auc_true` for the logistic model;

- `r auc_sub_6`, `r auc_sub_4`, and `r auc_sub_2` for the box of submodels;

- `r auc_rf_imp` for the random forest.
 

## Validation set

We use a sample size of 20000 in the validation sets, to make sure there are at least 2000 events in de val set, with the prevalence of ~0.15. 

```{r valset}
# generate complete validation set
valset <-
  generate_sample(
    sample_size = 20000,
    varcov,
    linear_betas,
    non_linear_betas,
    interaction = TRUE
  )
```

The number of events in this val set is `r sum(valset$Y)`. The next step is to ampute the validation set using several missing data patterns and missingness mechanisms. 

We use a mixture of three missing data patterns, with 40%, 60%, and 80% of variables being missing per case. The missingness mechanisms are a mixture as well: we combine the four types of MAR missingness ("MAR left", "MAR right", "MAR mid", and "MAR tail").

```{r missingness}
# define the missingness parameters
miss_par <- define_miss(p = nrow(varcov))
```

Ampute the validation set according to the missing data patterns and missingness mechanisms.

```{r ampute}
# make the validation set incomplete according to the missing data pattern and MAR types
valset <- create_miss(valset, miss_par)
# check missing data pattern
md <- mice::md.pattern(valset[,-c(1:2)])
```

# Estimands

Define estimands and/or other targets of the simulation study: 

- We calculate the absolute outcome risk according to different strategies for dealing with missing values.



# Methods

Identify methods to be evaluated and consider whether they are appropriate for estimand/target identified. 

Methods for prediction model development:

- Strategy 1: The prediction model is a flexible regression model including non-linear effects using RCS (restricted cubic splines) with 3 knots **check definition, use package `RCS`**. We also store the means and covariance of all predictor variables (using package `condmvnorm`, which can be used to generate imputations).

- Strategy 2: The prediction model is a "box" of submodels: a flexible  regression model is developed for each possible combination of available predictors. If we have 10 predictor variables, this means that we would have to fit 1  + 10 + 45 + 120 + 210 + 252 + 252 + 210 + 120 + 45  + 10 + 1  = 1276 regression models. However, there is no need to estimate all these models. We can first look in the hold-out sample what variable is missing, and then estimate the necessary "submodel". In our case, we estimate just 3 submodels (with 2, 4, or 6 variables observed according to the missing data pattern).

- Strategy 3: The prediction model is a random forest, as implemented by `cforest()` in the R package `party`. We generate a certain number of surrogate splits for each node. These splits attempt to mimic the primary split, and thus to achieve similar separation using another (observed) variable. By default set 4 surrogate splits (since we set max 3 missings). See "surrogate decisions based on additional variables (cf. Breiman et al., 1984; Hothorn et al., 2006)" (Hapfelmeier, p. 6). This is **very** computationally inefficient, so we'll need HPC power. Because in the clinical setting a 15 minute fitting procedure is not terrible, we do not scrap this method. However, we could also implement random forests in the presence of missing data using the `missranger` package. We'll add this method because it seems popular and efficient. We'll also use the joint method (Nijman et al.) to impute before fitting the `ranger` random forest.

Sidenote for strategy 3: Random forests are very susceptible to variations in the calibration slope, even in contexts without missingness. With huge devsets (i.e., `n_obs > 1.000.000`) or average performance across simulation repetitions, this problem fades. But we'll need to specifically check the calibration of the random forest models just to be sure!

Methods for generating absolute risk predictions:

- (Only for strategy 1 and 3): Missing values are imputed by their conditional mean.

- (Only for strategy 1 and 3): Missing values are imputed by a random draw from their conditional multivariate distribution.

- (Only for strategy 1 and 3): Missing values are imputed 50 times by a random draw from their distribution. The resulting 50 absolute risk predictions are then averaged to obtain the final prediction.

- (Only for strategy 2): The appropriate pattern submodel is selected for calculating an absolute risk.

- (Only for strategy 3): Missing values are handled using the surrogate splits.

```{r predict}
# generate predictions for each of the cases in the incomplete set
valset %>% glimpse() 
# this includes the observed values of the outcome (Y) and the number of missing predictor values, we'll exclude Y in the predictions

# start with easiest method to implement: box of submodels (strategy 2)
Y_pred_sub <- pred_sub(valset[,-2], submodel_2 = mod_sub_2, submodel_4 = mod_sub_4, submodel_6 = mod_sub_6)

# # predict with random forest models (switched off because of computational load)
# Y_pred_rf_imp <- pred_rf_imp(valset[,-c(1:2)], mod_rf_imp$mod)
# Y_pred_rf_sur <- predict(mod_rf_sur$mod, newdata = valset[,-c(1,2)])
# saveRDS(Y_pred_rf_sur, file = "Data/rf_sur.RDS")
Y_pred_rf_sur <- readRDS("Data/rf_sur.RDS")

# add mock predictions as placeholder for the conditional methods
Y_pred_cond <- rep(0, nrow(valset))

# combine into one dataframe
obs_vs_pred <- data.frame(Y_obs = valset[,"Y"],
                          Y_sub = Y_pred_sub,
                          Y_rf_sur = Y_pred_rf_sur,
                          Y_cond = Y_pred_cond)
```
Note for the placeholder data (always predict Y=0) is that there is no discrimination whatsoever.

# Performance measures

List all performance measures to be estimated, justifying their relevance to estimands or other targets. 

We evaluate the predicted outcome risk against the original outcome risk (i.e. the absolute outcome risk that would be obtained if we have no missing values). 
Calculate:

- Root mean square prediction error;

- Brier score (predicted risk vs observed outcome).

Finally, across all hold-out patients:

-	Calibration of (predicted Y | one or more missing predictor values) versus (predicted Y | original predictor values);

-	Discrimination of (predicted Y | one or more missing predictor values) versus (predicted Y | original predictor values);

-	Visual inspection of calibration plot (see plot Gary Collins in Stat med paper, min 100 events).

```{r performance}
# for the performance it will probably be nicer to make a list with the different methods and map over them
pred_list <- list(Y_sub = Y_pred_sub,
                  Y_rf_sur = Y_pred_rf_sur,
                  Y_cond = Y_pred_cond)

# RMSE
RMSE <- purrr::map(pred_list, ~ calc_RMSE(pred = ., obs = valset[, "Y"]))
RMSE

# brier score, for now separate from the RMSE but could be (way) more efficient
brier <- purrr::map(pred_list, ~ calc_brier(pred = ., obs = valset[, "Y"]))
brier
```
Add: brier with respect to **probability** of Y in valset.

